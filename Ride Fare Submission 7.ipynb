{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML - Taxi Fair 7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ_szTi5ENtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In this approach, I'm going to use SMOTE with Imbalance Data using imblearn module."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNvGN3WfDANO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "08831ad3-fb44-4127-b186-28dce8825035"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "import time\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "\n",
        "\n",
        "# Other Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ML Project Taxi Fair/train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tripid</th>\n",
              "      <th>additional_fare</th>\n",
              "      <th>duration</th>\n",
              "      <th>meter_waiting</th>\n",
              "      <th>meter_waiting_fare</th>\n",
              "      <th>meter_waiting_till_pickup</th>\n",
              "      <th>pickup_time</th>\n",
              "      <th>drop_time</th>\n",
              "      <th>pick_lat</th>\n",
              "      <th>pick_lon</th>\n",
              "      <th>drop_lat</th>\n",
              "      <th>drop_lon</th>\n",
              "      <th>fare</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>189123628</td>\n",
              "      <td>10.5</td>\n",
              "      <td>834.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>64.0</td>\n",
              "      <td>11/1/2019 0:20</td>\n",
              "      <td>11/1/2019 0:34</td>\n",
              "      <td>6.86252</td>\n",
              "      <td>79.8993</td>\n",
              "      <td>6.90330</td>\n",
              "      <td>79.8783</td>\n",
              "      <td>270.32</td>\n",
              "      <td>correct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>189125358</td>\n",
              "      <td>10.5</td>\n",
              "      <td>791.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>134.0</td>\n",
              "      <td>11/1/2019 0:56</td>\n",
              "      <td>11/1/2019 1:09</td>\n",
              "      <td>6.88589</td>\n",
              "      <td>79.8984</td>\n",
              "      <td>6.91373</td>\n",
              "      <td>79.8923</td>\n",
              "      <td>197.85</td>\n",
              "      <td>correct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>189125719</td>\n",
              "      <td>10.5</td>\n",
              "      <td>1087.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>61.0</td>\n",
              "      <td>11/1/2019 1:08</td>\n",
              "      <td>11/1/2019 1:26</td>\n",
              "      <td>6.90839</td>\n",
              "      <td>79.8651</td>\n",
              "      <td>6.93669</td>\n",
              "      <td>79.9146</td>\n",
              "      <td>301.64</td>\n",
              "      <td>correct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189127273</td>\n",
              "      <td>10.5</td>\n",
              "      <td>598.0</td>\n",
              "      <td>271.0</td>\n",
              "      <td>15.6638</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11/1/2019 2:27</td>\n",
              "      <td>11/1/2019 2:37</td>\n",
              "      <td>6.92570</td>\n",
              "      <td>79.8895</td>\n",
              "      <td>6.92748</td>\n",
              "      <td>79.8971</td>\n",
              "      <td>82.30</td>\n",
              "      <td>correct</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>189128020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11/1/2019 3:34</td>\n",
              "      <td>11/1/2019 3:51</td>\n",
              "      <td>6.87441</td>\n",
              "      <td>79.8615</td>\n",
              "      <td>6.84478</td>\n",
              "      <td>79.9290</td>\n",
              "      <td>358.39</td>\n",
              "      <td>correct</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      tripid  additional_fare  duration  ...  drop_lon    fare    label\n",
              "0  189123628             10.5     834.0  ...   79.8783  270.32  correct\n",
              "1  189125358             10.5     791.0  ...   79.8923  197.85  correct\n",
              "2  189125719             10.5    1087.0  ...   79.9146  301.64  correct\n",
              "3  189127273             10.5     598.0  ...   79.8971   82.30  correct\n",
              "4  189128020              NaN       NaN  ...   79.9290  358.39  correct\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvMqb7qyDosi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "98be1e5f-f5d7-459c-8418-067d2bd03c33"
      },
      "source": [
        "#Just check nan for refresh the memory. It's been 8 days since last submits. Things are getting blur on my memory.\n",
        "df.isna().sum()/df.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tripid                       0.000000\n",
              "additional_fare              1.176060\n",
              "duration                     1.176060\n",
              "meter_waiting                1.176060\n",
              "meter_waiting_fare           1.176060\n",
              "meter_waiting_till_pickup    1.176060\n",
              "pickup_time                  0.000000\n",
              "drop_time                    0.000000\n",
              "pick_lat                     0.000000\n",
              "pick_lon                     0.000000\n",
              "drop_lat                     0.000000\n",
              "drop_lon                     0.000000\n",
              "fare                         0.797625\n",
              "label                        0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zWAzsfbEfWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1% are related to nan values.I decided to remove the nan rows\n",
        "df.dropna(subset=['additional_fare','duration','meter_waiting','meter_waiting_fare','meter_waiting_till_pickup','pickup_time', 'fare'], inplace=True)\n",
        "df.drop(columns=['tripid'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EIHspe9Epe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "1e47cc4f-6d41-45cb-a9c5-873e738bac71"
      },
      "source": [
        "#Ok, NaN are removed now. Let's re assure it.\n",
        "df.isna().sum()/df.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "additional_fare              0.0\n",
              "duration                     0.0\n",
              "meter_waiting                0.0\n",
              "meter_waiting_fare           0.0\n",
              "meter_waiting_till_pickup    0.0\n",
              "pickup_time                  0.0\n",
              "drop_time                    0.0\n",
              "pick_lat                     0.0\n",
              "pick_lon                     0.0\n",
              "drop_lat                     0.0\n",
              "drop_lon                     0.0\n",
              "fare                         0.0\n",
              "label                        0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyzKpFDZHgjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode the label column\n",
        "\n",
        "def encode(x):\n",
        "  if x=='correct':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df['label'] = df['label'].apply(encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZHhLFi_G2w-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Oh I forgot to mentioned , I'm gonna delete the pickup_time and drop_time from dataframe\n",
        "df.drop(columns=['pickup_time','drop_time'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgR2pstJN8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "c05cb1bf-21aa-4f05-e854-724894fc1d5a"
      },
      "source": [
        "#The datapoint with values 12795.03 is a outlier\n",
        "#Find the index and remove it\n",
        "df[df.additional_fare > 2000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>additional_fare</th>\n",
              "      <th>duration</th>\n",
              "      <th>meter_waiting</th>\n",
              "      <th>meter_waiting_fare</th>\n",
              "      <th>meter_waiting_till_pickup</th>\n",
              "      <th>pick_lat</th>\n",
              "      <th>pick_lon</th>\n",
              "      <th>drop_lat</th>\n",
              "      <th>drop_lon</th>\n",
              "      <th>fare</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14043</th>\n",
              "      <td>12795.03</td>\n",
              "      <td>40.0</td>\n",
              "      <td>210164.0</td>\n",
              "      <td>12252.5612</td>\n",
              "      <td>182.0</td>\n",
              "      <td>7.15515</td>\n",
              "      <td>79.8718</td>\n",
              "      <td>7.15537</td>\n",
              "      <td>79.8799</td>\n",
              "      <td>25097.59</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       additional_fare  duration  meter_waiting  ...  drop_lon      fare  label\n",
              "14043         12795.03      40.0       210164.0  ...   79.8799  25097.59      0\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMYuVqDIJUlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Delete the index 14043\n",
        "df.drop([14043], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_gOgRFiJhSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "b8f53304-65a7-433e-9a52-f741e1e5d603"
      },
      "source": [
        "#The datapoint with values 48.132000 is a outlier\n",
        "#Find the index and remove it\n",
        "df[df.drop_lat > 40]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>additional_fare</th>\n",
              "      <th>duration</th>\n",
              "      <th>meter_waiting</th>\n",
              "      <th>meter_waiting_fare</th>\n",
              "      <th>meter_waiting_till_pickup</th>\n",
              "      <th>pick_lat</th>\n",
              "      <th>pick_lon</th>\n",
              "      <th>drop_lat</th>\n",
              "      <th>drop_lon</th>\n",
              "      <th>fare</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>920</th>\n",
              "      <td>10.5</td>\n",
              "      <td>313.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>2.915</td>\n",
              "      <td>98.0</td>\n",
              "      <td>7.46897</td>\n",
              "      <td>80.6227</td>\n",
              "      <td>48.132</td>\n",
              "      <td>45.3077</td>\n",
              "      <td>63.41</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     additional_fare  duration  meter_waiting  ...  drop_lon   fare  label\n",
              "920             10.5     313.0           50.0  ...   45.3077  63.41      1\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYOa9tMJxkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Delete the index 920\n",
        "df.drop([920], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8WvLLOKB3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert np.log and minmax transformation\n",
        "columns = [col for col in list(df.columns) if col!='label']\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler1 = StandardScaler()\n",
        "\n",
        "for col in columns:\n",
        "\n",
        "  #There weresome values which tranform inf or -inf in log scale, So add 1 for ech column\n",
        "  df[col] = df[col]+1\n",
        "  log_tranform = np.log(df[col].values.reshape(-1, 1))\n",
        "  scaler1.fit(log_tranform)\n",
        "  df[col] = scaler1.transform(log_tranform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKPfmKB2LmGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "4cacc43d-dff2-409e-9a35-4a6e6b8b431f"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>additional_fare</th>\n",
              "      <th>duration</th>\n",
              "      <th>meter_waiting</th>\n",
              "      <th>meter_waiting_fare</th>\n",
              "      <th>meter_waiting_till_pickup</th>\n",
              "      <th>pick_lat</th>\n",
              "      <th>pick_lon</th>\n",
              "      <th>drop_lat</th>\n",
              "      <th>drop_lon</th>\n",
              "      <th>fare</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.118493</td>\n",
              "      <td>-0.113264</td>\n",
              "      <td>-0.323926</td>\n",
              "      <td>-0.816484</td>\n",
              "      <td>0.168468</td>\n",
              "      <td>-0.471004</td>\n",
              "      <td>-0.406459</td>\n",
              "      <td>-0.280295</td>\n",
              "      <td>-0.497783</td>\n",
              "      <td>0.273197</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.118493</td>\n",
              "      <td>-0.164693</td>\n",
              "      <td>-0.419023</td>\n",
              "      <td>-0.816484</td>\n",
              "      <td>0.630810</td>\n",
              "      <td>-0.360140</td>\n",
              "      <td>-0.410181</td>\n",
              "      <td>-0.231168</td>\n",
              "      <td>-0.439910</td>\n",
              "      <td>-0.127441</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.118493</td>\n",
              "      <td>0.144185</td>\n",
              "      <td>-0.129474</td>\n",
              "      <td>-0.816484</td>\n",
              "      <td>0.138577</td>\n",
              "      <td>-0.253714</td>\n",
              "      <td>-0.547949</td>\n",
              "      <td>-0.123250</td>\n",
              "      <td>-0.347747</td>\n",
              "      <td>0.414043</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.118493</td>\n",
              "      <td>-0.436378</td>\n",
              "      <td>0.540849</td>\n",
              "      <td>0.898053</td>\n",
              "      <td>0.206245</td>\n",
              "      <td>-0.172043</td>\n",
              "      <td>-0.446997</td>\n",
              "      <td>-0.166502</td>\n",
              "      <td>-0.420070</td>\n",
              "      <td>-1.249235</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.118493</td>\n",
              "      <td>1.254842</td>\n",
              "      <td>0.321541</td>\n",
              "      <td>-0.816484</td>\n",
              "      <td>0.518283</td>\n",
              "      <td>0.797087</td>\n",
              "      <td>-0.416386</td>\n",
              "      <td>-0.208016</td>\n",
              "      <td>-0.553186</td>\n",
              "      <td>2.037416</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   additional_fare  duration  meter_waiting  ...  drop_lon      fare  label\n",
              "0        -0.118493 -0.113264      -0.323926  ... -0.497783  0.273197      1\n",
              "1        -0.118493 -0.164693      -0.419023  ... -0.439910 -0.127441      1\n",
              "2        -0.118493  0.144185      -0.129474  ... -0.347747  0.414043      1\n",
              "3        -0.118493 -0.436378       0.540849  ... -0.420070 -1.249235      1\n",
              "5        -0.118493  1.254842       0.321541  ... -0.553186  2.037416      1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUZcvhNvF3Eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a4d442ad-0c69-400c-fb49-bf1ff294626c"
      },
      "source": [
        "X = df.drop(columns=['label']).values\n",
        "y = df['label']\n",
        "print('Shape of X: {}'.format(X.shape))\n",
        "print('Shape of y: {}'.format(y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X: (16966, 10)\n",
            "Shape of y: (16966,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GunU9Pc2GmWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "eb07edcb-576b-4dfd-a194-6323364d7999"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(\"Number rides X_train dataset: \", X_train.shape)\n",
        "print(\"Number rides y_train dataset: \", y_train.shape)\n",
        "print(\"Number rides X_test dataset: \", X_test.shape)\n",
        "print(\"Number rides y_test dataset: \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number rides X_train dataset:  (11876, 10)\n",
            "Number rides y_train dataset:  (11876,)\n",
            "Number rides X_test dataset:  (5090, 10)\n",
            "Number rides y_test dataset:  (5090,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kguySDiFY_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "972d8e6e-e704-4fb0-9f9c-53df6913ae63"
      },
      "source": [
        "#So far so good. Nan are eleminated\n",
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
        "\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
        "\n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before OverSampling, counts of label '1': 10818\n",
            "Before OverSampling, counts of label '0': 1058 \n",
            "\n",
            "After OverSampling, the shape of train_X: (21636, 10)\n",
            "After OverSampling, the shape of train_y: (21636,) \n",
            "\n",
            "After OverSampling, counts of label '1': 10818\n",
            "After OverSampling, counts of label '0': 10818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2VNk9GEMIL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIQ90B8NGiL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "3573caa7-e738-4310-8ec1-d437b8a4344e"
      },
      "source": [
        "\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier(max_depth=30)))\n",
        "models.append(('NB', GaussianNB()))\n",
        "#models.append(('SVM', SVC(gamma=0.05, kernel='linear')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "\tkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\tcv_results = cross_val_score(model, X_train, y_train.ravel(), cv=kfold, scoring='f1')\n",
        "\tresults.append(cv_results)\n",
        "\tnames.append(name)\n",
        "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
        " \n",
        "sns.barplot(x=np.array(['LR','LDA','KNN','CART','NB']),y=[r.mean() for r in results])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR: 0.956503 (0.001109)\n",
            "LDA: 0.958166 (0.001536)\n",
            "KNN: 0.965656 (0.001667)\n",
            "CART: 0.953672 (0.003882)\n",
            "NB: 0.937273 (0.004521)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f50d1d068d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AADtSVchIMvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ANN\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Og_0j_IUVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65e5bd18-7b9d-4c81-c954-4cf469e86615"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# load the dataset\n",
        "#dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
        "# split into input (X) and output (y) variables\n",
        "\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=10, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "# fit the keras model on the dataset\n",
        "history = model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=1)\n",
        "# make class predictions with the model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "11876/11876 [==============================] - 2s 144us/step - loss: 0.3717 - acc: 0.8788 - f1_m: 0.9234 - precision_m: 0.9042 - recall_m: 0.9618\n",
            "Epoch 2/150\n",
            "11876/11876 [==============================] - 2s 135us/step - loss: 0.2531 - acc: 0.9171 - f1_m: 0.9540 - precision_m: 0.9192 - recall_m: 0.9963\n",
            "Epoch 3/150\n",
            "11876/11876 [==============================] - 2s 130us/step - loss: 0.2121 - acc: 0.9267 - f1_m: 0.9588 - precision_m: 0.9327 - recall_m: 0.9910\n",
            "Epoch 4/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1933 - acc: 0.9328 - f1_m: 0.9620 - precision_m: 0.9400 - recall_m: 0.9891\n",
            "Epoch 5/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1823 - acc: 0.9367 - f1_m: 0.9642 - precision_m: 0.9444 - recall_m: 0.9888\n",
            "Epoch 6/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1748 - acc: 0.9397 - f1_m: 0.9657 - precision_m: 0.9475 - recall_m: 0.9883\n",
            "Epoch 7/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1694 - acc: 0.9438 - f1_m: 0.9681 - precision_m: 0.9523 - recall_m: 0.9879\n",
            "Epoch 8/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1655 - acc: 0.9445 - f1_m: 0.9682 - precision_m: 0.9517 - recall_m: 0.9888\n",
            "Epoch 9/150\n",
            "11876/11876 [==============================] - 2s 127us/step - loss: 0.1627 - acc: 0.9457 - f1_m: 0.9690 - precision_m: 0.9536 - recall_m: 0.9882\n",
            "Epoch 10/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1605 - acc: 0.9464 - f1_m: 0.9695 - precision_m: 0.9546 - recall_m: 0.9881\n",
            "Epoch 11/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1595 - acc: 0.9468 - f1_m: 0.9698 - precision_m: 0.9554 - recall_m: 0.9876\n",
            "Epoch 12/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1573 - acc: 0.9470 - f1_m: 0.9697 - precision_m: 0.9553 - recall_m: 0.9878\n",
            "Epoch 13/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1564 - acc: 0.9484 - f1_m: 0.9705 - precision_m: 0.9580 - recall_m: 0.9867\n",
            "Epoch 14/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1551 - acc: 0.9483 - f1_m: 0.9706 - precision_m: 0.9553 - recall_m: 0.9895\n",
            "Epoch 15/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1545 - acc: 0.9482 - f1_m: 0.9704 - precision_m: 0.9556 - recall_m: 0.9887\n",
            "Epoch 16/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1532 - acc: 0.9495 - f1_m: 0.9712 - precision_m: 0.9572 - recall_m: 0.9888\n",
            "Epoch 17/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1521 - acc: 0.9489 - f1_m: 0.9711 - precision_m: 0.9569 - recall_m: 0.9887\n",
            "Epoch 18/150\n",
            "11876/11876 [==============================] - 1s 116us/step - loss: 0.1520 - acc: 0.9487 - f1_m: 0.9708 - precision_m: 0.9562 - recall_m: 0.9890\n",
            "Epoch 19/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1507 - acc: 0.9487 - f1_m: 0.9708 - precision_m: 0.9565 - recall_m: 0.9889\n",
            "Epoch 20/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1502 - acc: 0.9496 - f1_m: 0.9713 - precision_m: 0.9570 - recall_m: 0.9893\n",
            "Epoch 21/150\n",
            "11876/11876 [==============================] - 2s 127us/step - loss: 0.1496 - acc: 0.9502 - f1_m: 0.9714 - precision_m: 0.9572 - recall_m: 0.9892\n",
            "Epoch 22/150\n",
            "11876/11876 [==============================] - 1s 126us/step - loss: 0.1486 - acc: 0.9509 - f1_m: 0.9720 - precision_m: 0.9575 - recall_m: 0.9897\n",
            "Epoch 23/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1491 - acc: 0.9502 - f1_m: 0.9716 - precision_m: 0.9580 - recall_m: 0.9889\n",
            "Epoch 24/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1484 - acc: 0.9498 - f1_m: 0.9715 - precision_m: 0.9583 - recall_m: 0.9882\n",
            "Epoch 25/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1479 - acc: 0.9502 - f1_m: 0.9716 - precision_m: 0.9570 - recall_m: 0.9898\n",
            "Epoch 26/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1475 - acc: 0.9505 - f1_m: 0.9718 - precision_m: 0.9581 - recall_m: 0.9889\n",
            "Epoch 27/150\n",
            "11876/11876 [==============================] - 2s 126us/step - loss: 0.1475 - acc: 0.9501 - f1_m: 0.9715 - precision_m: 0.9573 - recall_m: 0.9892\n",
            "Epoch 28/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1472 - acc: 0.9506 - f1_m: 0.9717 - precision_m: 0.9573 - recall_m: 0.9896\n",
            "Epoch 29/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1469 - acc: 0.9508 - f1_m: 0.9718 - precision_m: 0.9595 - recall_m: 0.9874\n",
            "Epoch 30/150\n",
            "11876/11876 [==============================] - 1s 125us/step - loss: 0.1469 - acc: 0.9512 - f1_m: 0.9721 - precision_m: 0.9576 - recall_m: 0.9899\n",
            "Epoch 31/150\n",
            "11876/11876 [==============================] - 2s 128us/step - loss: 0.1465 - acc: 0.9508 - f1_m: 0.9719 - precision_m: 0.9585 - recall_m: 0.9889\n",
            "Epoch 32/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1464 - acc: 0.9511 - f1_m: 0.9720 - precision_m: 0.9586 - recall_m: 0.9889\n",
            "Epoch 33/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1456 - acc: 0.9507 - f1_m: 0.9717 - precision_m: 0.9580 - recall_m: 0.9889\n",
            "Epoch 34/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1458 - acc: 0.9514 - f1_m: 0.9723 - precision_m: 0.9584 - recall_m: 0.9897\n",
            "Epoch 35/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1455 - acc: 0.9515 - f1_m: 0.9724 - precision_m: 0.9593 - recall_m: 0.9891\n",
            "Epoch 36/150\n",
            "11876/11876 [==============================] - 1s 124us/step - loss: 0.1456 - acc: 0.9509 - f1_m: 0.9721 - precision_m: 0.9587 - recall_m: 0.9888\n",
            "Epoch 37/150\n",
            "11876/11876 [==============================] - 2s 134us/step - loss: 0.1454 - acc: 0.9508 - f1_m: 0.9719 - precision_m: 0.9589 - recall_m: 0.9887\n",
            "Epoch 38/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1455 - acc: 0.9511 - f1_m: 0.9720 - precision_m: 0.9587 - recall_m: 0.9888\n",
            "Epoch 39/150\n",
            "11876/11876 [==============================] - 1s 125us/step - loss: 0.1448 - acc: 0.9508 - f1_m: 0.9719 - precision_m: 0.9580 - recall_m: 0.9891\n",
            "Epoch 40/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1442 - acc: 0.9512 - f1_m: 0.9722 - precision_m: 0.9592 - recall_m: 0.9887\n",
            "Epoch 41/150\n",
            "11876/11876 [==============================] - 2s 128us/step - loss: 0.1443 - acc: 0.9520 - f1_m: 0.9728 - precision_m: 0.9593 - recall_m: 0.9896\n",
            "Epoch 42/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1448 - acc: 0.9510 - f1_m: 0.9719 - precision_m: 0.9588 - recall_m: 0.9885\n",
            "Epoch 43/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1442 - acc: 0.9527 - f1_m: 0.9728 - precision_m: 0.9593 - recall_m: 0.9898\n",
            "Epoch 44/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1444 - acc: 0.9513 - f1_m: 0.9724 - precision_m: 0.9593 - recall_m: 0.9886\n",
            "Epoch 45/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1435 - acc: 0.9514 - f1_m: 0.9722 - precision_m: 0.9588 - recall_m: 0.9889\n",
            "Epoch 46/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1437 - acc: 0.9516 - f1_m: 0.9724 - precision_m: 0.9590 - recall_m: 0.9890\n",
            "Epoch 47/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1440 - acc: 0.9520 - f1_m: 0.9724 - precision_m: 0.9588 - recall_m: 0.9892\n",
            "Epoch 48/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1437 - acc: 0.9512 - f1_m: 0.9722 - precision_m: 0.9589 - recall_m: 0.9891\n",
            "Epoch 49/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1431 - acc: 0.9518 - f1_m: 0.9723 - precision_m: 0.9586 - recall_m: 0.9895\n",
            "Epoch 50/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1436 - acc: 0.9518 - f1_m: 0.9725 - precision_m: 0.9595 - recall_m: 0.9886\n",
            "Epoch 51/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1429 - acc: 0.9511 - f1_m: 0.9722 - precision_m: 0.9583 - recall_m: 0.9896\n",
            "Epoch 52/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1438 - acc: 0.9514 - f1_m: 0.9723 - precision_m: 0.9591 - recall_m: 0.9890\n",
            "Epoch 53/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1430 - acc: 0.9506 - f1_m: 0.9717 - precision_m: 0.9582 - recall_m: 0.9886\n",
            "Epoch 54/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1426 - acc: 0.9517 - f1_m: 0.9723 - precision_m: 0.9586 - recall_m: 0.9895\n",
            "Epoch 55/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1426 - acc: 0.9516 - f1_m: 0.9722 - precision_m: 0.9587 - recall_m: 0.9893\n",
            "Epoch 56/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1424 - acc: 0.9517 - f1_m: 0.9724 - precision_m: 0.9592 - recall_m: 0.9890\n",
            "Epoch 57/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1427 - acc: 0.9523 - f1_m: 0.9730 - precision_m: 0.9595 - recall_m: 0.9895\n",
            "Epoch 58/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1425 - acc: 0.9520 - f1_m: 0.9725 - precision_m: 0.9586 - recall_m: 0.9901\n",
            "Epoch 59/150\n",
            "11876/11876 [==============================] - 2s 129us/step - loss: 0.1426 - acc: 0.9516 - f1_m: 0.9722 - precision_m: 0.9588 - recall_m: 0.9893\n",
            "Epoch 60/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1421 - acc: 0.9520 - f1_m: 0.9726 - precision_m: 0.9588 - recall_m: 0.9898\n",
            "Epoch 61/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1416 - acc: 0.9518 - f1_m: 0.9723 - precision_m: 0.9587 - recall_m: 0.9892\n",
            "Epoch 62/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1417 - acc: 0.9510 - f1_m: 0.9722 - precision_m: 0.9598 - recall_m: 0.9879\n",
            "Epoch 63/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1419 - acc: 0.9519 - f1_m: 0.9725 - precision_m: 0.9588 - recall_m: 0.9898\n",
            "Epoch 64/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1416 - acc: 0.9528 - f1_m: 0.9729 - precision_m: 0.9602 - recall_m: 0.9889\n",
            "Epoch 65/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1412 - acc: 0.9532 - f1_m: 0.9733 - precision_m: 0.9600 - recall_m: 0.9899\n",
            "Epoch 66/150\n",
            "11876/11876 [==============================] - 1s 126us/step - loss: 0.1420 - acc: 0.9522 - f1_m: 0.9728 - precision_m: 0.9596 - recall_m: 0.9892\n",
            "Epoch 67/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1418 - acc: 0.9519 - f1_m: 0.9727 - precision_m: 0.9591 - recall_m: 0.9896\n",
            "Epoch 68/150\n",
            "11876/11876 [==============================] - 1s 125us/step - loss: 0.1417 - acc: 0.9511 - f1_m: 0.9721 - precision_m: 0.9583 - recall_m: 0.9895\n",
            "Epoch 69/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1410 - acc: 0.9518 - f1_m: 0.9724 - precision_m: 0.9591 - recall_m: 0.9894\n",
            "Epoch 70/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1411 - acc: 0.9519 - f1_m: 0.9724 - precision_m: 0.9588 - recall_m: 0.9896\n",
            "Epoch 71/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1415 - acc: 0.9529 - f1_m: 0.9732 - precision_m: 0.9602 - recall_m: 0.9894\n",
            "Epoch 72/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1415 - acc: 0.9525 - f1_m: 0.9729 - precision_m: 0.9601 - recall_m: 0.9888\n",
            "Epoch 73/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1415 - acc: 0.9534 - f1_m: 0.9732 - precision_m: 0.9598 - recall_m: 0.9897\n",
            "Epoch 74/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1410 - acc: 0.9534 - f1_m: 0.9734 - precision_m: 0.9599 - recall_m: 0.9900\n",
            "Epoch 75/150\n",
            "11876/11876 [==============================] - 2s 131us/step - loss: 0.1413 - acc: 0.9523 - f1_m: 0.9724 - precision_m: 0.9591 - recall_m: 0.9894\n",
            "Epoch 76/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1414 - acc: 0.9514 - f1_m: 0.9721 - precision_m: 0.9594 - recall_m: 0.9883\n",
            "Epoch 77/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1401 - acc: 0.9519 - f1_m: 0.9724 - precision_m: 0.9589 - recall_m: 0.9893\n",
            "Epoch 78/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1407 - acc: 0.9534 - f1_m: 0.9736 - precision_m: 0.9602 - recall_m: 0.9903\n",
            "Epoch 79/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1407 - acc: 0.9519 - f1_m: 0.9728 - precision_m: 0.9594 - recall_m: 0.9894\n",
            "Epoch 80/150\n",
            "11876/11876 [==============================] - 2s 131us/step - loss: 0.1407 - acc: 0.9517 - f1_m: 0.9724 - precision_m: 0.9587 - recall_m: 0.9895\n",
            "Epoch 81/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1405 - acc: 0.9523 - f1_m: 0.9727 - precision_m: 0.9590 - recall_m: 0.9899\n",
            "Epoch 82/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1404 - acc: 0.9525 - f1_m: 0.9730 - precision_m: 0.9600 - recall_m: 0.9891\n",
            "Epoch 83/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1408 - acc: 0.9519 - f1_m: 0.9727 - precision_m: 0.9589 - recall_m: 0.9898\n",
            "Epoch 84/150\n",
            "11876/11876 [==============================] - 2s 127us/step - loss: 0.1397 - acc: 0.9522 - f1_m: 0.9726 - precision_m: 0.9588 - recall_m: 0.9899\n",
            "Epoch 85/150\n",
            "11876/11876 [==============================] - 2s 131us/step - loss: 0.1405 - acc: 0.9531 - f1_m: 0.9732 - precision_m: 0.9615 - recall_m: 0.9883\n",
            "Epoch 86/150\n",
            "11876/11876 [==============================] - 2s 129us/step - loss: 0.1401 - acc: 0.9536 - f1_m: 0.9735 - precision_m: 0.9596 - recall_m: 0.9905\n",
            "Epoch 87/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1402 - acc: 0.9521 - f1_m: 0.9724 - precision_m: 0.9597 - recall_m: 0.9885\n",
            "Epoch 88/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1403 - acc: 0.9526 - f1_m: 0.9730 - precision_m: 0.9592 - recall_m: 0.9900\n",
            "Epoch 89/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1404 - acc: 0.9525 - f1_m: 0.9727 - precision_m: 0.9600 - recall_m: 0.9891\n",
            "Epoch 90/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1391 - acc: 0.9523 - f1_m: 0.9729 - precision_m: 0.9590 - recall_m: 0.9898\n",
            "Epoch 91/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1403 - acc: 0.9528 - f1_m: 0.9733 - precision_m: 0.9595 - recall_m: 0.9904\n",
            "Epoch 92/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1395 - acc: 0.9525 - f1_m: 0.9729 - precision_m: 0.9603 - recall_m: 0.9888\n",
            "Epoch 93/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1399 - acc: 0.9531 - f1_m: 0.9733 - precision_m: 0.9598 - recall_m: 0.9901\n",
            "Epoch 94/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1395 - acc: 0.9531 - f1_m: 0.9732 - precision_m: 0.9601 - recall_m: 0.9894\n",
            "Epoch 95/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1397 - acc: 0.9524 - f1_m: 0.9726 - precision_m: 0.9589 - recall_m: 0.9897\n",
            "Epoch 96/150\n",
            "11876/11876 [==============================] - 1s 118us/step - loss: 0.1398 - acc: 0.9530 - f1_m: 0.9732 - precision_m: 0.9615 - recall_m: 0.9883\n",
            "Epoch 97/150\n",
            "11876/11876 [==============================] - 1s 116us/step - loss: 0.1395 - acc: 0.9528 - f1_m: 0.9730 - precision_m: 0.9595 - recall_m: 0.9898\n",
            "Epoch 98/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1391 - acc: 0.9534 - f1_m: 0.9736 - precision_m: 0.9602 - recall_m: 0.9902\n",
            "Epoch 99/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1385 - acc: 0.9537 - f1_m: 0.9734 - precision_m: 0.9603 - recall_m: 0.9897\n",
            "Epoch 100/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1393 - acc: 0.9531 - f1_m: 0.9733 - precision_m: 0.9599 - recall_m: 0.9902\n",
            "Epoch 101/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1395 - acc: 0.9528 - f1_m: 0.9730 - precision_m: 0.9604 - recall_m: 0.9886\n",
            "Epoch 102/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1391 - acc: 0.9527 - f1_m: 0.9730 - precision_m: 0.9604 - recall_m: 0.9886\n",
            "Epoch 103/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1393 - acc: 0.9533 - f1_m: 0.9734 - precision_m: 0.9597 - recall_m: 0.9904\n",
            "Epoch 104/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1386 - acc: 0.9527 - f1_m: 0.9730 - precision_m: 0.9603 - recall_m: 0.9890\n",
            "Epoch 105/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1389 - acc: 0.9538 - f1_m: 0.9735 - precision_m: 0.9609 - recall_m: 0.9894\n",
            "Epoch 106/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1392 - acc: 0.9531 - f1_m: 0.9732 - precision_m: 0.9599 - recall_m: 0.9899\n",
            "Epoch 107/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1394 - acc: 0.9523 - f1_m: 0.9728 - precision_m: 0.9603 - recall_m: 0.9886\n",
            "Epoch 108/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1389 - acc: 0.9531 - f1_m: 0.9732 - precision_m: 0.9603 - recall_m: 0.9895\n",
            "Epoch 109/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1385 - acc: 0.9531 - f1_m: 0.9732 - precision_m: 0.9601 - recall_m: 0.9898\n",
            "Epoch 110/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1386 - acc: 0.9527 - f1_m: 0.9728 - precision_m: 0.9594 - recall_m: 0.9898\n",
            "Epoch 111/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1390 - acc: 0.9533 - f1_m: 0.9733 - precision_m: 0.9611 - recall_m: 0.9888\n",
            "Epoch 112/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1381 - acc: 0.9531 - f1_m: 0.9731 - precision_m: 0.9606 - recall_m: 0.9888\n",
            "Epoch 113/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1380 - acc: 0.9529 - f1_m: 0.9731 - precision_m: 0.9596 - recall_m: 0.9899\n",
            "Epoch 114/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1385 - acc: 0.9523 - f1_m: 0.9730 - precision_m: 0.9600 - recall_m: 0.9894\n",
            "Epoch 115/150\n",
            "11876/11876 [==============================] - 1s 126us/step - loss: 0.1382 - acc: 0.9530 - f1_m: 0.9731 - precision_m: 0.9593 - recall_m: 0.9903\n",
            "Epoch 116/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1384 - acc: 0.9528 - f1_m: 0.9731 - precision_m: 0.9608 - recall_m: 0.9885\n",
            "Epoch 117/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1384 - acc: 0.9525 - f1_m: 0.9728 - precision_m: 0.9593 - recall_m: 0.9900\n",
            "Epoch 118/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1381 - acc: 0.9530 - f1_m: 0.9732 - precision_m: 0.9605 - recall_m: 0.9894\n",
            "Epoch 119/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1384 - acc: 0.9525 - f1_m: 0.9728 - precision_m: 0.9610 - recall_m: 0.9880\n",
            "Epoch 120/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1378 - acc: 0.9527 - f1_m: 0.9729 - precision_m: 0.9603 - recall_m: 0.9888\n",
            "Epoch 121/150\n",
            "11876/11876 [==============================] - 2s 129us/step - loss: 0.1379 - acc: 0.9534 - f1_m: 0.9735 - precision_m: 0.9603 - recall_m: 0.9900\n",
            "Epoch 122/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1377 - acc: 0.9538 - f1_m: 0.9735 - precision_m: 0.9609 - recall_m: 0.9896\n",
            "Epoch 123/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1376 - acc: 0.9537 - f1_m: 0.9737 - precision_m: 0.9609 - recall_m: 0.9897\n",
            "Epoch 124/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1384 - acc: 0.9533 - f1_m: 0.9733 - precision_m: 0.9606 - recall_m: 0.9893\n",
            "Epoch 125/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1378 - acc: 0.9531 - f1_m: 0.9732 - precision_m: 0.9601 - recall_m: 0.9894\n",
            "Epoch 126/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1379 - acc: 0.9528 - f1_m: 0.9730 - precision_m: 0.9608 - recall_m: 0.9886\n",
            "Epoch 127/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1389 - acc: 0.9523 - f1_m: 0.9728 - precision_m: 0.9603 - recall_m: 0.9885\n",
            "Epoch 128/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1374 - acc: 0.9535 - f1_m: 0.9735 - precision_m: 0.9620 - recall_m: 0.9884\n",
            "Epoch 129/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1377 - acc: 0.9532 - f1_m: 0.9732 - precision_m: 0.9607 - recall_m: 0.9891\n",
            "Epoch 130/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1379 - acc: 0.9535 - f1_m: 0.9735 - precision_m: 0.9607 - recall_m: 0.9894\n",
            "Epoch 131/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1374 - acc: 0.9534 - f1_m: 0.9735 - precision_m: 0.9602 - recall_m: 0.9902\n",
            "Epoch 132/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1377 - acc: 0.9536 - f1_m: 0.9735 - precision_m: 0.9614 - recall_m: 0.9888\n",
            "Epoch 133/150\n",
            "11876/11876 [==============================] - 1s 119us/step - loss: 0.1376 - acc: 0.9528 - f1_m: 0.9728 - precision_m: 0.9604 - recall_m: 0.9888\n",
            "Epoch 134/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1377 - acc: 0.9528 - f1_m: 0.9730 - precision_m: 0.9600 - recall_m: 0.9891\n",
            "Epoch 135/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1378 - acc: 0.9534 - f1_m: 0.9733 - precision_m: 0.9607 - recall_m: 0.9892\n",
            "Epoch 136/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1379 - acc: 0.9528 - f1_m: 0.9732 - precision_m: 0.9608 - recall_m: 0.9887\n",
            "Epoch 137/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1377 - acc: 0.9531 - f1_m: 0.9733 - precision_m: 0.9601 - recall_m: 0.9897\n",
            "Epoch 138/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1375 - acc: 0.9528 - f1_m: 0.9730 - precision_m: 0.9611 - recall_m: 0.9883\n",
            "Epoch 139/150\n",
            "11876/11876 [==============================] - 1s 125us/step - loss: 0.1373 - acc: 0.9538 - f1_m: 0.9736 - precision_m: 0.9607 - recall_m: 0.9897\n",
            "Epoch 140/150\n",
            "11876/11876 [==============================] - 2s 128us/step - loss: 0.1367 - acc: 0.9537 - f1_m: 0.9735 - precision_m: 0.9613 - recall_m: 0.9892\n",
            "Epoch 141/150\n",
            "11876/11876 [==============================] - 2s 142us/step - loss: 0.1371 - acc: 0.9532 - f1_m: 0.9732 - precision_m: 0.9606 - recall_m: 0.9893\n",
            "Epoch 142/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1364 - acc: 0.9524 - f1_m: 0.9727 - precision_m: 0.9602 - recall_m: 0.9886\n",
            "Epoch 143/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1375 - acc: 0.9530 - f1_m: 0.9731 - precision_m: 0.9601 - recall_m: 0.9893\n",
            "Epoch 144/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1371 - acc: 0.9531 - f1_m: 0.9733 - precision_m: 0.9608 - recall_m: 0.9891\n",
            "Epoch 145/150\n",
            "11876/11876 [==============================] - 1s 117us/step - loss: 0.1372 - acc: 0.9536 - f1_m: 0.9732 - precision_m: 0.9611 - recall_m: 0.9887\n",
            "Epoch 146/150\n",
            "11876/11876 [==============================] - 1s 120us/step - loss: 0.1371 - acc: 0.9531 - f1_m: 0.9733 - precision_m: 0.9612 - recall_m: 0.9884\n",
            "Epoch 147/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1363 - acc: 0.9538 - f1_m: 0.9738 - precision_m: 0.9613 - recall_m: 0.9896\n",
            "Epoch 148/150\n",
            "11876/11876 [==============================] - 1s 121us/step - loss: 0.1371 - acc: 0.9534 - f1_m: 0.9733 - precision_m: 0.9613 - recall_m: 0.9885\n",
            "Epoch 149/150\n",
            "11876/11876 [==============================] - 1s 122us/step - loss: 0.1361 - acc: 0.9539 - f1_m: 0.9737 - precision_m: 0.9608 - recall_m: 0.9898\n",
            "Epoch 150/150\n",
            "11876/11876 [==============================] - 1s 123us/step - loss: 0.1368 - acc: 0.9536 - f1_m: 0.9736 - precision_m: 0.9610 - recall_m: 0.9895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNnjje7GO4VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Acc of 0.9500 training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb304SSfIowz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create sample submission file using ANN train f1 : 0.9736\n",
        "\n",
        "#Testing frame\n",
        "test_frame = pd.read_csv(\"/content/drive/My Drive/ML Project Taxi Fair/test.csv\")\n",
        "trip_ids = test_frame['tripid']\n",
        "\n",
        "test_frame.drop(columns=['tripid','pickup_time','drop_time'], inplace=True)\n",
        "\n",
        "#From now .I try to transform every column to np.log => MinMax Scaler\n",
        "\n",
        "new_dt_sample = {}\n",
        "\n",
        "for column in test_frame.columns:\n",
        "  values = np.log((test_frame[column]+1).values).reshape(-1, 1)\n",
        "\n",
        "  scaler1 = StandardScaler()\n",
        "  scaler1.fit(np.log((test_frame[column]+1).values).reshape(-1, 1))\n",
        "  values = scaler1.transform(np.log((test_frame[column]+1).values).reshape(-1, 1))\n",
        "  values = [x[0] for x in values]\n",
        "  new_dt_sample[column] = values\n",
        "\n",
        "new_df_sample = pd.DataFrame(new_dt_sample)\n",
        "\n",
        "X_test = new_df_sample.values\n",
        "predicted = model.predict_classes(X_test)\n",
        "\n",
        "data_dict = { 'tripid':trip_ids, 'prediction':[x[0] for x in predicted]}\n",
        "pd_test_result = pd.DataFrame(data=data_dict)\n",
        "\n",
        "pd_test_result.to_csv('/content/drive/My Drive/ML Project Taxi Fair/sample_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0HhCr_cN68e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Accuracy of 0.97345 submission"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}